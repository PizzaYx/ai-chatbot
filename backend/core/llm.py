import os
from llama_index.llms.openai import OpenAI
from llama_index.llms.openai_like import OpenAILike
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding

# å…¨å±€ç¼“å­˜
_llm_instance = None
_embed_instance = None
_current_config_id = None  # ç”¨äºè®°å½•å½“å‰ä½¿ç”¨çš„é…ç½®IDï¼Œæ£€æµ‹å˜æ›´

def init_llm():
    """
    åˆå§‹åŒ–ä¸é…ç½®å…¨å±€ LLM è®¾ç½®
    ä¼˜å…ˆä»æ•°æ®åº“ (System App) è·å–é…ç½®ï¼Œæ— é…ç½®åˆ™å›é€€åˆ°ç¯å¢ƒå˜é‡
    æ”¯æŒçƒ­æ›´æ–°ï¼šå¦‚æœæ•°æ®åº“é…ç½®å‘ç”Ÿå˜åŒ–ï¼Œä¼šè‡ªåŠ¨é‡æ–°åŠ è½½
    """
    global _llm_instance, _current_config_id
    
    # 1. å°è¯•ä»æ•°æ®åº“è·å–æ´»è·ƒé…ç½®
    db_config = None
    try:
        from system.models import LLMConfig
        # é¿å…åœ¨ Django åˆå§‹åŒ–å®Œæˆå‰è°ƒç”¨å¯¼è‡´ AppRegistryNotReady
        import django
        if django.apps.apps.ready:
            db_config = LLMConfig.objects.filter(is_active=True).first()
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–æ•°æ®åº“ LLM é…ç½®: {e}")

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
    # å¦‚æœæœ‰æ•°æ®åº“é…ç½®
    if db_config:
        # å¦‚æœç¼“å­˜IDä¸å½“å‰IDä¸åŒï¼Œæˆ–è€…æ²¡æœ‰ç¼“å­˜å®ä¾‹ï¼Œåˆ™é‡æ–°åˆå§‹åŒ–
        config_signature = f"{db_config.id}_{db_config.updated_at}"
        if _current_config_id != config_signature:
            print(f"ğŸ”„ æ£€æµ‹åˆ° LLM é…ç½®å˜æ›´ï¼Œæ­£åœ¨é‡æ–°åŠ è½½: {db_config.name}")
            _llm_instance = None # å¼ºåˆ¶é‡ç½®
            _current_config_id = config_signature
            
            api_key = db_config.api_key
            api_base = db_config.base_url
            model_name = db_config.model_name
        else:
            # é…ç½®æœªå˜ï¼Œç›´æ¥è¿”å›ç¼“å­˜
            return _llm_instance
            
    # å¦‚æœæ²¡æœ‰æ•°æ®åº“é…ç½® (å›é€€æ¨¡å¼)
    elif _llm_instance is None:
        # åªæœ‰åœ¨æ²¡æœ‰ç¼“å­˜å®ä¾‹æ—¶æ‰ä»ç¯å¢ƒå˜é‡åŠ è½½
        print("â„¹ï¸ æœªæ£€æµ‹åˆ°æ•°æ®åº“ LLM é…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡å›é€€æ¨¡å¼")
        api_key = os.getenv("OPENAI_API_KEY")
        api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        model_name = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
        _current_config_id = "ENV"
    else:
        # ä¹‹å‰æ˜¯ç¯å¢ƒå˜é‡æ¨¡å¼ä¸”è¿˜æ²¡å˜
        return _llm_instance
    
    if not api_key:
        print("âš ï¸  WARNING: API_KEY æœªè®¾ç½®ï¼ŒAI åŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚")
        return None

    # 3. é…ç½® Embedding æ¨¡å‹ (ä¿æŒå•ä¾‹)
    embed_model = init_embedding()

    # 4. é…ç½® LLM (å¤§æ¨¡å‹)
    if not api_base or "api.openai.com" in api_base:
        llm = OpenAI(
            model=model_name,
            api_key=api_key,
            api_base=api_base, # OpenAI åŸç”Ÿä¹Ÿæ”¯æŒ base_urlï¼Œä½†é€šå¸¸ä¸éœ€è¦
            temperature=0.1,
            max_tokens=2048
        )
    else:
        llm = OpenAILike(
            model=model_name,
            api_key=api_key,
            api_base=api_base,
            is_chat_model=True,
            context_window=4096,
            temperature=0.1,
            max_tokens=2048
        )

    # 5. ç»‘å®šåˆ°å…¨å±€ Settings
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    _llm_instance = llm
    print(f"âœ… LLM æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model_name}")
    return llm


def init_embedding():
    """
    åˆå§‹åŒ– Embedding æ¨¡å‹
    ä½¿ç”¨è½»é‡çº§çš„ BGE-small-zhï¼ˆä¸­æ–‡ä¸“ç²¾ï¼Œä½“ç§¯å°ï¼‰
    ä½¿ç”¨å•ä¾‹æ¨¡å¼ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
    """
    global _embed_instance
    
    if _embed_instance is not None:
        return _embed_instance
    
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    
    # BGE-small-zhï¼šä½“ç§¯å°ï¼ˆ~100MBï¼‰ã€ä¸­æ–‡æ•ˆæœå¥½ã€å†…å­˜å ç”¨ä½
    model_name = "BAAI/bge-small-zh-v1.5"
    
    # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œé¿å…è”ç½‘æ£€æŸ¥å¯¼è‡´çš„è¶…æ—¶
    os.environ["HF_HUB_OFFLINE"] = "1"
    
    print(f"â³ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹: {model_name}...")
    _embed_instance = HuggingFaceEmbedding(
        model_name=model_name,
        embed_batch_size=10,
    )
    print("âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ (å·²ç¼“å­˜)")
    return _embed_instance

